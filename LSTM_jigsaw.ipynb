{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_jigsaw.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SiweiLuo/DL_with_Python/blob/master/LSTM_jigsaw.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ejQKiA_QEbrh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### credit by\n",
        "#\n",
        "# Engineer at Ierae security Inc.\n",
        "# Tokyo, Japan\n",
        "# Joined a year ago · last seen in the past day\n",
        "# https://nama.ne.jp/\n",
        "# Tanrei(nama)\n",
        "#\n",
        "###\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add concatenate\n",
        "from keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.losses import binary_crossentropy\n",
        "from keras import backend as K \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yo3tNLmkE-0l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EMBEDDING_FILES = []\n",
        "NUM_MODELS = 2 \n",
        "BATCH_SIZE = 512 \n",
        "LSTM_UNITS = 128 \n",
        "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
        "EPOCHS = 4 \n",
        "MAX_LEN = 220 \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kI1GX3G-FO4G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_coefs(word, *arr):\n",
        "  return word, np.asarray(arr,dtype='float32')\n",
        "\n",
        "def load_embeddings(path):\n",
        "  with open(path) as f: \n",
        "    return dict(get_coefs(*line.strip().split(' ')) for line in f)\n",
        "  \n",
        "def build_matrix(word_index, path):\n",
        "  embedding_index = load_embeddings(path)\n",
        "  embedding_matrix = np.zeros((len(word_index)+1,300))\n",
        "  for word, i in word_index.items():\n",
        "    try:\n",
        "      embedding_matrix[i] = embedding_index[word]\n",
        "    except KeyError:\n",
        "      pass\n",
        "  return embedding_matrix\n",
        "\n",
        "def custom_loss(y_true,y_pred):\n",
        "  return binary_crossentropy(K.reshape(y_true[:,0],(-1,1)),y_pred)*y_true[:,1]\n",
        "\n",
        "\n",
        "def build_model(embedding_matrix, num_aux_targets, loss_weight):\n",
        "  words = Input(shape=(MAX_LEN,))\n",
        "  x = Embedding(*embedding_matrix.shape,weights=[embedding_matrix],trainable=False)(words)\n",
        "  x = SpatialDropout1D(0.3)(x)\n",
        "  x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
        "  x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
        "  \n",
        "  hidden = concatenate([\n",
        "      GlobalMaxPooling1D()(x),\n",
        "      GlobalAveragePooling1D()(x),\n",
        "  ])\n",
        "  \n",
        "  hidden = add([hidden,Dense(DENSE_HIDDEN_UNITS,activation='relu')(hidden)])\n",
        "  hidden = add([hidden,Dense(DENSE_HIDDEN_UNITS,activation='relu')(hidden)])\n",
        "  \n",
        "  result = Dense(1,activation='sigmoid')(hidden)\n",
        "  \n",
        "  aux_result = Dense(num_aux_targets,activation='sigmoid')(hidden)\n",
        "  \n",
        "  model = Model(inputs=words,outputs=[result,aux_result])\n",
        "  \n",
        "  model.compile(loss=[custom_loss,'binary_crossentropy'],loss_weights=[loss_weight,1.0],optimizer='adam')\n",
        "  \n",
        "  return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OSIZ2GCFH7-y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess(data):\n",
        "  '''\n",
        "  Credit goes to https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n",
        "  '''\n",
        "  punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
        "  \n",
        "  def clean_special_chars(text,punct):\n",
        "    for p in punct:\n",
        "      text = text.replace(p,' ')\n",
        "    return text\n",
        "  \n",
        "  data = data.astype(str).apply(lambda x: clean_special_chars(x,punct))\n",
        "  \n",
        "  return data\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9evGhjXPIWoY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_csv()\n",
        "test = pd.read_csv()\n",
        "\n",
        "x_train = preprocess(train['comment_text'])\n",
        "\n",
        "identity_columns = [\n",
        "    'male','female','homosexual_gay_or_lesbian','christian','jewish','muslin','black',\n",
        "    'white','psychiatric_or_mental_illness'\n",
        "]\n",
        "\n",
        "weights = np.ones((len(x_train),)) / 4 \n",
        "\n",
        "weights += (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n",
        "\n",
        "weights += (( (train['target'].values>=0.5).astype(bool).astype(np.int) +\n",
        "   (train[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n",
        "\n",
        "weights += (( (train['target'].values<0.5).astype(bool).astype(np.int) +\n",
        "   (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n",
        "\n",
        "loss_weight = 1.0 / weights.mean()\n",
        "\n",
        "y_train = np.vstack([(train['target'].values>=0.5).astype(np.int),weights]).T\n",
        "y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']].values\n",
        "x_test = preprocess(test['comment_text'])\n",
        "\n",
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(list(x_train) + list(x_test))\n",
        "\n",
        "x_train = tokenizer.texts_to_sequences(x_train)\n",
        "x_test = tokenizer.texts_to_sequences(x_test)\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)\n",
        "\n",
        "\n",
        "embedding_matrix = np.concatenate(\n",
        "    [build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)\n",
        "\n",
        "import pickle\n",
        "import gc\n",
        "\n",
        "\n",
        "with open('temporary.pickle', mode='wb') as f:\n",
        "    pickle.dump(x_test, f) # use temporary file to reduce memory\n",
        "\n",
        "del identity_columns, weights, tokenizer, train, test, x_test\n",
        "gc.collect()\n",
        "\n",
        "checkpoint_predictions = []\n",
        "weights = []\n",
        "\n",
        "for model_idx in range(NUM_MODELS):\n",
        "    model = build_model(embedding_matrix, y_aux_train.shape[-1], loss_weight)\n",
        "    for global_epoch in range(EPOCHS):\n",
        "        model.fit(\n",
        "            x_train,\n",
        "            [y_train, y_aux_train],\n",
        "            batch_size=BATCH_SIZE,\n",
        "            epochs=1,\n",
        "            verbose=1,\n",
        "            callbacks=[\n",
        "                LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch))\n",
        "            ]\n",
        "        )\n",
        "        with open('temporary.pickle', mode='rb') as f:\n",
        "            x_test = pickle.load(f) # use temporary file to reduce memory\n",
        "        checkpoint_predictions.append(model.predict(x_test, batch_size=2048)[0].flatten())\n",
        "        del x_test\n",
        "        gc.collect()\n",
        "        weights.append(2 ** global_epoch)\n",
        "    del model\n",
        "    gc.collect()\n",
        "      \n",
        "predictions = np.average(checkpoint_predictions, weights=weights, axis=0)\n",
        "\n",
        "df_submit = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')\n",
        "df_submit.prediction = predictions\n",
        "df_submit.to_csv('submission.csv', index=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}